# data/papers.yaml
# Source of truth for Diffusion Lighthouse.
# This list is human-curated. Citation counts are added separately.

papers:

  # ===== Foundations =====
  - id: ddpm_2020
    title: "Denoising Diffusion Probabilistic Models"
    year: 2020
    authors: ["Ho", "Jain", "Abbeel"]
    venue: "NeurIPS"
    links:
      arxiv: "https://arxiv.org/abs/2006.11239"
      pdf: "https://arxiv.org/pdf/2006.11239.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=622631041436591387&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Denoising Diffusion Probabilistic Models\" Ho Jain Abbeel"
    tags: ["foundations", "image"]
    impact_type: foundational
    why_it_matters: >
      Introduced diffusion probabilistic models as a practical generative modeling framework,
      showing that iterative denoising from Gaussian noise can achieve competitive image synthesis.
      Established the core training objective and sampling procedure that form the basis of most
      subsequent diffusion models.

  - id: ddim_2020
    title: "Denoising Diffusion Implicit Models"
    year: 2020
    authors: ["Song", "Meng", "Ermon"]
    venue: "ICLR"
    links:
      arxiv: "https://arxiv.org/abs/2010.02502"
      pdf: "https://arxiv.org/pdf/2010.02502.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=15692403916484267912&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Denoising Diffusion Implicit Models\""
    tags: ["foundations", "sampling"]
    impact_type: enabling
    relations:
      - type: improves
        target: ddpm_2020
    why_it_matters: >
      Demonstrated that diffusion models can be sampled using a non-Markovian process that
      preserves sample quality while requiring far fewer steps. This significantly improved
      inference efficiency without retraining and made diffusion models more practical in real
      applications.

  - id: score_sde_2021
    title: "Score-Based Generative Modeling through Stochastic Differential Equations"
    year: 2021
    authors: ["Song", "Sohl-Dickstein", "Kingma", "Kumar", "Ermon", "Poole"]
    venue: "ICLR"
    links:
      arxiv: "https://arxiv.org/abs/2011.13456"
      pdf: "https://arxiv.org/pdf/2011.13456.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=14592788616550656262&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Score-Based Generative Modeling through Stochastic Differential Equations\""
    tags: ["foundations", "theory"]
    impact_type: foundational
    relations:
      - type: unifies
        target: score_matching_2019
      - type: extends
        target: ddpm_2020
    why_it_matters: >
      Unified score-based generative models and diffusion processes under a continuous-time
      stochastic differential equation framework. Provided theoretical clarity and enabled new
      sampling methods, connecting discrete diffusion models with broader probabilistic modeling
      theory.

  - id: score_matching_2019
    title: "Generative Modeling by Estimating Gradients of the Data Distribution"
    year: 2019
    authors: ["Song", "Ermon"]
    venue: "NeurIPS"
    links:
      arxiv: "https://arxiv.org/abs/1907.05600"
      pdf: "https://arxiv.org/pdf/1907.05600.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=7819543055117584506&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Generative Modeling by Estimating Gradients of the Data Distribution\""
    tags: ["foundations", "score"]
    impact_type: foundational
    relations:
      - type: precedes
        target: ddpm_2020
    why_it_matters: >
      Introduced score matching as a scalable approach for generative modeling, enabling models
      to learn gradients of the data distribution directly. This work laid the conceptual and
      mathematical groundwork for later score-based diffusion models.

  # ===== Training & Objectives =====
  - id: improved_ddpm_2021
    title: "Improved Denoising Diffusion Probabilistic Models"
    year: 2021
    authors: ["Nichol", "Dhariwal"]
    venue: "ICML"
    links:
      arxiv: "https://arxiv.org/abs/2102.09672"
      pdf: "https://arxiv.org/pdf/2102.09672.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=1314010070205781055&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Improved Denoising Diffusion Probabilistic Models\""
    tags: ["training", "image"]
    impact_type: refinement
    relations:
      - type: improves
        target: ddpm_2020
    why_it_matters: >
      Improved diffusion training and sampling through better noise schedules,
      parameterization, and loss weighting. These refinements significantly increased
      sample quality and stability and became standard practice in later diffusion models.

  # ===== Guidance & Conditioning =====
  - id: classifier_guidance_2021
    title: "Diffusion Models Beat GANs on Image Synthesis"
    year: 2021
    authors: ["Dhariwal", "Nichol"]
    venue: "NeurIPS"
    links:
      arxiv: "https://arxiv.org/abs/2105.05233"
      pdf: "https://arxiv.org/pdf/2105.05233.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=17982230494456470673&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Diffusion Models Beat GANs on Image Synthesis\""
    tags: ["guidance", "image"]
    impact_type: enabling
    why_it_matters: >
      Demonstrated state-of-the-art class-conditional image synthesis with diffusion models and
      introduced classifier guidance as a practical way to trade off sample fidelity and diversity.
      This helped establish diffusion as a competitive (and later dominant) paradigm over GANs
      for high-fidelity image generation.

  - id: classifier_free_guidance_2022
    title: "Classifier-Free Diffusion Guidance"
    year: 2022
    authors: ["Ho", "Salimans"]
    venue: "arXiv"
    links:
      arxiv: "https://arxiv.org/abs/2207.12598"
      pdf: "https://arxiv.org/pdf/2207.12598.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=9321084442049185729&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Classifier-Free Diffusion Guidance\""
    tags: ["guidance", "conditioning"]
    impact_type: enabling
    why_it_matters: >
      Proposed classifier-free guidance, enabling strong conditional generation without training a
      separate classifier by mixing conditional and unconditional predictions during sampling.
      This became a default technique in text-to-image systems for boosting prompt adherence with
      a single model.

  # ===== Latent & Scaling =====
  - id: latent_diffusion_2022
    title: "High-Resolution Image Synthesis with Latent Diffusion Models"
    year: 2022
    authors: ["Rombach", "Blattmann", "Lorenz", "Esser", "Ommer"]
    venue: "CVPR"
    links:
      arxiv: "https://arxiv.org/abs/2112.10752"
      pdf: "https://arxiv.org/pdf/2112.10752.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=2427242760668866618&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"High-Resolution Image Synthesis with Latent Diffusion Models\""
    tags: ["latent", "image", "systems"]
    impact_type: enabling
    why_it_matters: >
      Moved diffusion to a learned latent space to dramatically reduce computation and memory while
      retaining high perceptual quality, enabling practical high-resolution generation on commodity
      hardware. This design underpins many widely-used text-to-image pipelines and made large-scale
      diffusion deployment far more feasible.

  # ===== Text-to-Image =====
  - id: glide_2021
    title: "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"
    year: 2021
    authors: ["Nichol", "Dhariwal", "Ramesh", "et al."]
    venue: "arXiv"
    links:
      arxiv: "https://arxiv.org/abs/2112.10741"
      pdf: "https://arxiv.org/pdf/2112.10741.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=15472303808406531445&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"GLIDE\" text-guided diffusion models"
    tags: ["text-to-image", "editing"]
    impact_type: enabling
    why_it_matters: >
      Showed that diffusion models can be effectively conditioned on text for both generation and
      image editing, establishing core recipes for text-guided diffusion before the big wave of
      production text-to-image systems. Helped crystallize “prompted diffusion” as a general-purpose
      controllable generation approach.

  - id: imagen_2022
    title: "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
    year: 2022
    authors: ["Saharia", "Chan", "Saxena", "et al."]
    venue: "arXiv"
    links:
      arxiv: "https://arxiv.org/abs/2205.11487"
      pdf: "https://arxiv.org/pdf/2205.11487.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=2130901831690841916&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\""
    tags: ["text-to-image", "scaling"]
    impact_type: enabling
    why_it_matters: >
      Demonstrated that strong text encoders and large-scale training substantially improve text
      alignment and photorealism in diffusion-based text-to-image generation. Popularized the idea
      that “language understanding” (not just image modeling) is a key lever for text-to-image quality.

  # ===== Acceleration =====
  - id: progressive_distillation_2022
    title: "Progressive Distillation for Fast Sampling of Diffusion Models"
    year: 2022
    authors: ["Salimans", "Ho"]
    venue: "ICLR"
    links:
      arxiv: "https://arxiv.org/abs/2202.00512"
      pdf: "https://arxiv.org/pdf/2202.00512.pdf"
    scholar:
      scholar_url: "https://scholar.google.com/scholar?cluster=5194434213555432016&hl=en&as_sdt=2005&sciodt=0,5"
      scholar_query: "\"Progressive Distillation for Fast Sampling of Diffusion Models\""
    tags: ["acceleration", "distillation"]
    impact_type: enabling
    why_it_matters: >
      Introduced a practical distillation approach that progressively reduces the number of sampling
      steps while maintaining quality, making diffusion inference significantly faster. This work is
      a cornerstone for later “few-step” diffusion approaches and production-oriented acceleration.